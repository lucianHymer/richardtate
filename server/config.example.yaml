# Server configuration
server:
  # HTTP server bind address (use "localhost:8080" for local only, "0.0.0.0:8080" for LAN access)
  bind_address: "localhost:8080"

  # Enable debug logging
  debug: true

# WebRTC configuration
webrtc:
  # ICE servers for connection establishment
  # For localhost testing, this can be empty
  # For LAN/remote clients, configure STUN/TURN servers
  ice_servers: []
  # Example:
  # ice_servers:
  #   - urls: ["stun:stun.l.google.com:19302"]

# Transcription configuration
transcription:
  # Whisper model path (relative to server binary or absolute)
  model_path: "./models/ggml-large-v3-turbo.bin"

  # Language (leave empty for auto-detect)
  language: ""

  # Enable translation to English
  translate: false

  # Number of threads (0 = auto-detect)
  threads: 0

  # Use GPU if available (Metal on Mac, CUDA on Linux with GPU)
  use_gpu: true

# Noise suppression
noise_suppression:
  # Enable RNNoise
  enabled: true

  # RNNoise model path
  model_path: "./models/rnnoise/lq.rnnn"

# Voice Activity Detection (VAD)
vad:
  # Enable VAD-based chunking
  enabled: true

  # Energy threshold for speech detection (higher = requires louder speech)
  # Range: 100-1000, default: 500
  energy_threshold: 500.0

  # Silence duration (ms) to trigger chunk boundary
  # Chunk on 1 second of continuous silence
  silence_threshold_ms: 1000

  # Minimum chunk duration (ms) - avoid very short chunks
  min_chunk_duration_ms: 500

  # Maximum chunk duration (ms) - safety limit to prevent unbounded buffering
  max_chunk_duration_ms: 30000
